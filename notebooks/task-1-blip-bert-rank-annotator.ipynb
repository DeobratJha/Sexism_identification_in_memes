{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10849832,"sourceType":"datasetVersion","datasetId":6738447},{"sourceId":11119897,"sourceType":"datasetVersion","datasetId":6934106},{"sourceId":11128806,"sourceType":"datasetVersion","datasetId":6940569},{"sourceId":11759474,"sourceType":"datasetVersion","datasetId":7382311},{"sourceId":11839510,"sourceType":"datasetVersion","datasetId":7438594},{"sourceId":11839587,"sourceType":"datasetVersion","datasetId":7438644},{"sourceId":11839645,"sourceType":"datasetVersion","datasetId":7438689},{"sourceId":11840020,"sourceType":"datasetVersion","datasetId":7438958}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install transformers sentencepiece accelerate datasets torchvision tqdm deep-translator\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:18:56.928348Z","iopub.execute_input":"2025-05-10T18:18:56.929191Z","iopub.status.idle":"2025-05-10T18:18:56.932944Z","shell.execute_reply.started":"2025-05-10T18:18:56.929158Z","shell.execute_reply":"2025-05-10T18:18:56.932269Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom transformers import AutoTokenizer, AutoModel, ViTModel\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nimport pandas as pd\nimport os\n\n################################# every time #################################3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:18:56.933822Z","iopub.execute_input":"2025-05-10T18:18:56.934006Z","iopub.status.idle":"2025-05-10T18:19:20.415735Z","shell.execute_reply.started":"2025-05-10T18:18:56.933992Z","shell.execute_reply":"2025-05-10T18:19:20.415164Z"}},"outputs":[{"name":"stderr","text":"2025-05-10 18:19:09.691291: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746901149.873864      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746901149.929817      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Load Dataset\ndf = pd.read_csv(\"/kaggle/input/preprocessed-data-2103-new/preprocessed_2103.csv\") \ndf['label'] = df['final_labels_task2_1'].map({'YES': 1, 'NO': 0})\n\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:19:20.417062Z","iopub.execute_input":"2025-05-10T18:19:20.417679Z","iopub.status.idle":"2025-05-10T18:19:20.495455Z","shell.execute_reply.started":"2025-05-10T18:19:20.417657Z","shell.execute_reply":"2025-05-10T18:19:20.494785Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0  id_EXIST lang  \\\n0           0    110001   es   \n1           1    110002   es   \n2           2    110003   es   \n3           3    110005   es   \n4           4    110006   es   \n\n                                                text         meme  \\\n0  2+2=5 MITO Albert Einstein ten√≠a bajo rendimie...  110001.jpeg   \n1     CUANDO UNA MUJER VA A LUCHAR POR SUS DERECHOS   110002.jpeg   \n2  –Ü–Ø –ï–ì–ï–Ø –ï MOA ¬øEl Partido Republicano busca pe...  110003.jpeg   \n3  Ya ver√°s como este 8 de marzo hay uno que te s...  110005.jpeg   \n4  Princesa, hoy es tu d√≠a, no laves ning√∫n plato...  110006.jpeg   \n\n                                      path_memes  number_annotators  \\\n0  /kaggle/input/memes-dataset/memes/110001.jpeg                  6   \n1  /kaggle/input/memes-dataset/memes/110002.jpeg                  6   \n2  /kaggle/input/memes-dataset/memes/110003.jpeg                  6   \n3  /kaggle/input/memes-dataset/memes/110005.jpeg                  6   \n4  /kaggle/input/memes-dataset/memes/110006.jpeg                  6   \n\n                                          annotators  \\\n0  ['Annotator_1', 'Annotator_2', 'Annotator_3', ...   \n1  ['Annotator_1', 'Annotator_2', 'Annotator_3', ...   \n2  ['Annotator_1', 'Annotator_2', 'Annotator_3', ...   \n3  ['Annotator_1', 'Annotator_2', 'Annotator_3', ...   \n4  ['Annotator_1', 'Annotator_2', 'Annotator_3', ...   \n\n                               labels_task2_1 final_labels_task2_1  label  \n0  ['YES', 'YES', 'YES', 'YES', 'YES', 'YES']                  YES      1  \n1  ['YES', 'YES', 'YES', 'YES', 'YES', 'YES']                  YES      1  \n2      ['YES', 'YES', 'NO', 'NO', 'NO', 'NO']                   NO      0  \n3      ['NO', 'YES', 'NO', 'NO', 'YES', 'NO']                   NO      0  \n4  ['YES', 'YES', 'YES', 'YES', 'YES', 'YES']                  YES      1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>id_EXIST</th>\n      <th>lang</th>\n      <th>text</th>\n      <th>meme</th>\n      <th>path_memes</th>\n      <th>number_annotators</th>\n      <th>annotators</th>\n      <th>labels_task2_1</th>\n      <th>final_labels_task2_1</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>110001</td>\n      <td>es</td>\n      <td>2+2=5 MITO Albert Einstein ten√≠a bajo rendimie...</td>\n      <td>110001.jpeg</td>\n      <td>/kaggle/input/memes-dataset/memes/110001.jpeg</td>\n      <td>6</td>\n      <td>['Annotator_1', 'Annotator_2', 'Annotator_3', ...</td>\n      <td>['YES', 'YES', 'YES', 'YES', 'YES', 'YES']</td>\n      <td>YES</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>110002</td>\n      <td>es</td>\n      <td>CUANDO UNA MUJER VA A LUCHAR POR SUS DERECHOS</td>\n      <td>110002.jpeg</td>\n      <td>/kaggle/input/memes-dataset/memes/110002.jpeg</td>\n      <td>6</td>\n      <td>['Annotator_1', 'Annotator_2', 'Annotator_3', ...</td>\n      <td>['YES', 'YES', 'YES', 'YES', 'YES', 'YES']</td>\n      <td>YES</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>110003</td>\n      <td>es</td>\n      <td>–Ü–Ø –ï–ì–ï–Ø –ï MOA ¬øEl Partido Republicano busca pe...</td>\n      <td>110003.jpeg</td>\n      <td>/kaggle/input/memes-dataset/memes/110003.jpeg</td>\n      <td>6</td>\n      <td>['Annotator_1', 'Annotator_2', 'Annotator_3', ...</td>\n      <td>['YES', 'YES', 'NO', 'NO', 'NO', 'NO']</td>\n      <td>NO</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>110005</td>\n      <td>es</td>\n      <td>Ya ver√°s como este 8 de marzo hay uno que te s...</td>\n      <td>110005.jpeg</td>\n      <td>/kaggle/input/memes-dataset/memes/110005.jpeg</td>\n      <td>6</td>\n      <td>['Annotator_1', 'Annotator_2', 'Annotator_3', ...</td>\n      <td>['NO', 'YES', 'NO', 'NO', 'YES', 'NO']</td>\n      <td>NO</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>110006</td>\n      <td>es</td>\n      <td>Princesa, hoy es tu d√≠a, no laves ning√∫n plato...</td>\n      <td>110006.jpeg</td>\n      <td>/kaggle/input/memes-dataset/memes/110006.jpeg</td>\n      <td>6</td>\n      <td>['Annotator_1', 'Annotator_2', 'Annotator_3', ...</td>\n      <td>['YES', 'YES', 'YES', 'YES', 'YES', 'YES']</td>\n      <td>YES</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"df['label'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:19:20.496039Z","iopub.execute_input":"2025-05-10T18:19:20.496232Z","iopub.status.idle":"2025-05-10T18:19:20.504585Z","shell.execute_reply.started":"2025-05-10T18:19:20.496216Z","shell.execute_reply":"2025-05-10T18:19:20.503764Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"label\n1    2369\n0    1675\nName: count, dtype: int64"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"Step 2: Translate Spanish Text to English","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nmin_count = df[\"lang\"].value_counts().min()  # Get minimum count across languages\ndf_balanced = df.groupby(\"lang\").apply(lambda x: x.sample(min_count)).reset_index(drop=True)  #Ensuring Equal Samples for \"en\" & \"es\"\n\nfrom sklearn.model_selection import train_test_split\n\n# Split Data: 85% Training, 15% Validation\ntrain_df, val_df = train_test_split(\n    df_balanced, \n    test_size=0.15, \n    stratify=df_balanced[\"label\"],   #Ensures that both training and validation sets maintain the same proportion of each label/class.\n    random_state=42\n)\n\n# Check the shape of the splits\nprint(\"Train set shape:\", train_df.shape)\nprint(\"Validation set shape:\", val_df.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:19:20.506223Z","iopub.execute_input":"2025-05-10T18:19:20.506753Z","iopub.status.idle":"2025-05-10T18:19:20.545012Z","shell.execute_reply.started":"2025-05-10T18:19:20.506735Z","shell.execute_reply":"2025-05-10T18:19:20.544208Z"}},"outputs":[{"name":"stdout","text":"Train set shape: (3417, 11)\nValidation set shape: (603, 11)\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3585815532.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_balanced = df.groupby(\"lang\").apply(lambda x: x.sample(min_count)).reset_index(drop=True)  #Ensuring Equal Samples for \"en\" & \"es\"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":" Step 3: Load BLIP & BERT","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n###################################################################################### every time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:19:20.545812Z","iopub.execute_input":"2025-05-10T18:19:20.546120Z","iopub.status.idle":"2025-05-10T18:19:20.549966Z","shell.execute_reply.started":"2025-05-10T18:19:20.546092Z","shell.execute_reply":"2025-05-10T18:19:20.549128Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:19:20.550775Z","iopub.execute_input":"2025-05-10T18:19:20.551642Z","iopub.status.idle":"2025-05-10T18:19:20.565734Z","shell.execute_reply.started":"2025-05-10T18:19:20.551616Z","shell.execute_reply":"2025-05-10T18:19:20.565120Z"}},"outputs":[{"name":"stdout","text":"4.51.1\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification,BlipProcessor, BlipForConditionalGeneration\nfrom torch.optim import AdamW  # correct for transformers 4.51.1\n\n\n\nblip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nblip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\nbert_model = AutoModel.from_pretrained(\"bert-base-multilingual-cased\").to(device)\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n\n\n############################################3############################################# every time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:19:20.566297Z","iopub.execute_input":"2025-05-10T18:19:20.566539Z","iopub.status.idle":"2025-05-10T18:19:42.322948Z","shell.execute_reply.started":"2025-05-10T18:19:20.566523Z","shell.execute_reply":"2025-05-10T18:19:42.322360Z"}},"outputs":[{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4432b0ad6ab40908f31c04838ee5753"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79862a02a1ed438d9c7ffb0ae8e5e28a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5568c96931144c42acd1fb1b1addbd47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5abaeb58416345398520d9be36d26edd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec8f85b97bcb4b09b2eacecff3a08a56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d03dfe8d8a547b2960e6fd440f6f76e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"122377d11a3e49c28dee490bcf40d465"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17d0ef933bfa442da9c4023b4e3f382f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd38bfb90b2b4838b3d12c5b98ed5fc9"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48a686f42de44285b2b165738e0d4647"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4dace2482244697a447f3cc0c6b482f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"084bb22de5fc44c3bff0cdac36582511"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d276606ab5d47ee9c01ba889bbc38f3"}},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"Step 4: Prepare BERT Dataset","metadata":{}},{"cell_type":"code","source":"# 1a) Tokenizer for original + generated text\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\ndef process_text(batch_texts):\n    \"\"\"Tokenize a list of strings for BERT.\"\"\"\n    if isinstance(batch_texts, str):\n        batch_texts = [batch_texts]\n    encoded = tokenizer(\n        batch_texts,\n        padding=\"max_length\",\n        truncation=True,\n        max_length=96,\n        return_tensors=\"pt\"\n    )\n    return {k: v.to(device) for k, v in encoded.items()}\n\n# 1b) BLIP processor for images + caption generation\nblip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\ndef process_image(image_path):\n    \"\"\"Load & preprocess a single image for BLIP (pixel_values).\"\"\"\n    img = Image.open(image_path).convert(\"RGB\")\n    return blip_processor(images=img, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:19:42.323512Z","iopub.execute_input":"2025-05-10T18:19:42.323780Z","iopub.status.idle":"2025-05-10T18:19:46.399873Z","shell.execute_reply.started":"2025-05-10T18:19:42.323756Z","shell.execute_reply":"2025-05-10T18:19:46.399100Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a6be2ddc66a4cf8a2370eccc43b37fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6e0c20d7aa64fafa686af0b9f7a5da9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01c5bd197b8f4addaf683670f7c32857"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"147217e3fff54b1898af839f37c8d415"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# from PIL import Image\n# import torchvision.transforms as transforms\n# def process_image(image_path):\n#     \"\"\"Loads and preprocesses the image for BLIP model.\"\"\"\n#     image = Image.open(image_path).convert(\"RGB\")\n#     return blip_processor(images=image, return_tensors=\"pt\")[\"pixoncatenate the three vectors‚Äî(1) vision CLS, (2) caption embedding, and (3) original-text embedding‚Äîalong the feature dimension.el_values\"].squeeze(0)  # Remove batch dimension","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:19:46.400707Z","iopub.execute_input":"2025-05-10T18:19:46.400912Z","iopub.status.idle":"2025-05-10T18:19:46.404389Z","shell.execute_reply.started":"2025-05-10T18:19:46.400896Z","shell.execute_reply":"2025-05-10T18:19:46.403567Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# ‚îÄ‚îÄ‚îÄ Phase 2: Dataset & Collate fn ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nclass MemeDataset(Dataset):\n    def __init__(self, df):\n        self.df = df\n        # optional extra transforms (e.g., normalization) can go here\n        self.resize = transforms.Resize((224, 224))\n    \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_tensor = process_image(row[\"path_memes\"])  # [3,H,W]\n        # You could also do extra transforms: img_tensor = self.resize(img_tensor)\n        text = row[\"text\"]\n        label = torch.tensor(row[\"label\"], dtype=torch.long)\n        return {\"image\": img_tensor.to(device),\n                \"text\": text,\n                \"label\": label.to(device)}\n\ndef meme_collate_fn(batch):\n    images = torch.stack([item[\"image\"] for item in batch], dim=0)  # [B,3,H,W]\n    texts  = [item[\"text\"]  for item in batch]                     # list[str]\n    labels = torch.stack([item[\"label\"] for item in batch], dim=0) # [B]\n    # we delay BERT tokenization until inside model for generated captions,\n    # but we still need to tokenize original texts now\n    text_inputs = process_text(texts)\n    return {\"image\": images, \"text_inputs\": text_inputs, \"raw_texts\": texts, \"label\": labels}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:19:46.407174Z","iopub.execute_input":"2025-05-10T18:19:46.407512Z","iopub.status.idle":"2025-05-10T18:19:57.183136Z","shell.execute_reply.started":"2025-05-10T18:19:46.407489Z","shell.execute_reply":"2025-05-10T18:19:57.182534Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# ‚îÄ‚îÄ‚îÄ Phase 3: Model Definition (UPDATED) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nclass AttentionPooling(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        self.attn = nn.Linear(hidden_dim, 1)\n    def forward(self, x):\n        # x: [B, L, H]\n        weights = torch.softmax(self.attn(x), dim=1)  # [B, L, 1]\n        return torch.sum(weights * x, dim=1)          # [B, H]\n\nclass MemeClassifier(nn.Module):\n    def __init__(self, num_classes=2):\n        super().__init__()\n        # BLIP captioner\n        self.blip = BlipForConditionalGeneration.from_pretrained(\n            \"Salesforce/blip-image-captioning-base\"\n        ).to(device)\n        # BERT encoder\n        self.bert = AutoModel.from_pretrained(\"bert-base-multilingual-cased\").to(device)\n\n        # allow full fine-tuning\n        for p in self.blip.parameters(): p.requires_grad = True\n        for p in self.bert.parameters(): p.requires_grad = True\n\n        # dims\n        vh = self.blip.vision_model.config.hidden_size\n        th = self.bert.config.hidden_size\n        self.vision_hid = vh\n        self.text_hid   = th\n\n        # attention pooling for BERT outputs\n        self.attn_pool = AttentionPooling(th)\n\n        # final classifier: [ vision_cls + cap_BERT + orig_BERT ] ‚Üí classes\n        total_dim = vh + 2 * th\n        self.classifier = nn.Sequential(\n            nn.Linear(total_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, images, text_inputs, raw_texts):\n        # 1) Vision CLS\n        vis = self.blip.vision_model(pixel_values=images).last_hidden_state\n        img_feat = vis[:, 0, :]  # [B, vh]\n\n        # 2) Caption generation (no_grad + eval, shorter beams)\n        self.blip.eval()\n        with torch.no_grad():\n            gen_ids = self.blip.generate(\n                pixel_values=images,\n                max_length=16,\n                num_beams=1,\n                early_stopping=True\n            )\n        self.blip.train()\n\n        caps = blip_processor.batch_decode(gen_ids, skip_special_tokens=True)\n\n        # 3) BERT encode captions\n        cap_inputs = tokenizer(\n            caps, padding=True, truncation=True, max_length=96, return_tensors=\"pt\"\n        ).to(device)\n        cap_hid  = self.bert(**cap_inputs).last_hidden_state\n        cap_feat = self.attn_pool(cap_hid)\n\n        # 4) BERT encode original meme text\n        orig_hid  = self.bert(**text_inputs).last_hidden_state\n        orig_feat = self.attn_pool(orig_hid)\n\n        # 5) concat & classify\n        combined = torch.cat([img_feat, cap_feat, orig_feat], dim=1)\n        return self.classifier(combined)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:19:57.183930Z","iopub.execute_input":"2025-05-10T18:19:57.184188Z","iopub.status.idle":"2025-05-10T18:19:57.196449Z","shell.execute_reply.started":"2025-05-10T18:19:57.184165Z","shell.execute_reply":"2025-05-10T18:19:57.195877Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# class MemeClassifier(nn.Module):\n#     def __init__(self, num_classes=2):\n#         super(MemeClassifier, self).__init__()\n#         self.blip = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n#         self.bert = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n        \n#         # Enable fine-tuning of BLIP and BERT (Unfreeze their parameters)\n#         for param in self.blip.parameters():\n#             param.requires_grad = True  # üîπ Allow BLIP fine-tuning\n#         for param in self.bert.parameters():\n#             param.requires_grad = True  # üîπ Allow BERT fine-tuning\n\n#         # Hidden sizes\n#         self.vision_hidden_size = self.blip.vision_model.config.hidden_size\n#         self.text_hidden_size = self.bert.config.hidden_size\n        \n#         # Attention pooling over BERT\n#         self.attn_pooling = AttentionPooling(self.text_hidden_size)\n\n#         # Classifier layers\n#         self.fc = nn.Sequential(\n#             nn.Linear(self.vision_hidden_size + self.text_hidden_size, 512),\n#             nn.ReLU(),\n#             nn.Dropout(0.5),\n#             nn.Linear(512, num_classes)\n#         )\n\n#     def forward(self, image, text_inputs):\n#         # Extract BLIP image features\n#         vision_outputs = self.blip.vision_model(image)\n#         image_features = vision_outputs.last_hidden_state[:, 0, :]  # Take CLS token\n\n#         # Extract BERT text features\n#         bert_output = self.bert(**text_inputs)['last_hidden_state']\n#         text_features = self.attn_pooling(bert_output)\n\n#         # Concatenate & classify\n#         combined = torch.cat((image_features, text_features), dim=1)\n#         logits = self.fc(combined)\n#         return logits\n# ######################################################################## every time #########################################################","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:19:57.197165Z","iopub.execute_input":"2025-05-10T18:19:57.197441Z","iopub.status.idle":"2025-05-10T18:19:57.212873Z","shell.execute_reply.started":"2025-05-10T18:19:57.197417Z","shell.execute_reply":"2025-05-10T18:19:57.212196Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# # Custom Collate Function\n# def meme_collate_fn(batch):\n#     images = torch.stack([item['image'] for item in batch])  # shape: [B, C, H, W]\n#     texts = [item['text'] for item in batch]  # list of strings\n#     labels = torch.stack([item['label'] for item in batch])  # shape: [B]\n\n#     text_inputs = process_text(texts)  # tokenize all at once\n\n#     return {\n#         \"image\": images,\n#         \"text\": text_inputs,\n#         \"label\": labels\n#     }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:19:57.213612Z","iopub.execute_input":"2025-05-10T18:19:57.213777Z","iopub.status.idle":"2025-05-10T18:19:57.229810Z","shell.execute_reply.started":"2025-05-10T18:19:57.213764Z","shell.execute_reply":"2025-05-10T18:19:57.229092Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# ‚îÄ‚îÄ‚îÄ Phase 4: DataLoaders, Loss & Optimizer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\ntrain_loader = DataLoader(MemeDataset(train_df),\n                          batch_size=8, shuffle=True,\n                          collate_fn=meme_collate_fn)\nval_loader   = DataLoader(MemeDataset(val_df),\n                          batch_size=8, shuffle=False,\n                          collate_fn=meme_collate_fn)\n\nmodel = MemeClassifier(num_classes=2).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = AdamW([\n    {\"params\": model.blip.parameters(),   \"lr\": 5e-6},\n    {\"params\": model.bert.parameters(),   \"lr\": 5e-6},\n    {\"params\": model.classifier.parameters(), \"lr\": 1e-4},\n], weight_decay=0.01)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:19:57.230576Z","iopub.execute_input":"2025-05-10T18:19:57.230830Z","iopub.status.idle":"2025-05-10T18:19:59.419629Z","shell.execute_reply.started":"2025-05-10T18:19:57.230812Z","shell.execute_reply":"2025-05-10T18:19:59.419054Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# train_loader = DataLoader(MemeDataset(train_df), batch_size=8, shuffle=True, collate_fn=meme_collate_fn)\n# val_loader = DataLoader(MemeDataset(val_df), batch_size=8, shuffle=False, collate_fn=meme_collate_fn)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:19:59.420357Z","iopub.execute_input":"2025-05-10T18:19:59.420546Z","iopub.status.idle":"2025-05-10T18:19:59.423875Z","shell.execute_reply.started":"2025-05-10T18:19:59.420531Z","shell.execute_reply":"2025-05-10T18:19:59.423087Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# ‚îÄ‚îÄ‚îÄ Phase 5: Training & Validation Loop ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\ncheckpoint_dir = \"/kaggle/working\"\nos.makedirs(checkpoint_dir, exist_ok=True)\n\nbest_val_acc = 0.0\n\n# Lists to store training & validation accuracy/loss\ntrain_losses, val_losses = [], []\ntrain_accuracies, val_accuracies = [], []\n\nfor epoch in range(1, 12):\n    # ‚Äî Training ‚Äî\n    model.train()\n    total_loss, correct, total = 0.0, 0, 0\n\n    for batch in tqdm(train_loader, desc=f\"[Train] Epoch {epoch}\", leave=False):\n        imgs = batch[\"image\"]\n        ti   = batch[\"text_inputs\"]\n        rt   = batch[\"raw_texts\"]\n        lbls = batch[\"label\"]\n\n        optimizer.zero_grad()\n        logits = model(imgs, ti, rt)\n        loss   = criterion(logits, lbls)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n\n        total_loss += loss.item()\n        correct    += (logits.argmax(1) == lbls).sum().item()\n        total      += lbls.size(0)\n\n    avg_train_loss = total_loss / len(train_loader)\n    train_acc      = correct / total\n\n    train_losses.append(avg_train_loss)\n    train_accuracies.append(train_acc)\n    print(f\"Epoch {epoch} Train Loss: {avg_train_loss:.4f} Acc: {train_acc:.2%}\")\n\n    # ‚Äî Validation ‚Äî\n    model.eval()\n    v_loss, v_corr, v_tot = 0.0, 0, 0\n\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=f\"[Val]   Epoch {epoch}\", leave=False):\n            logits = model(batch[\"image\"],\n                           batch[\"text_inputs\"],\n                           batch[\"raw_texts\"])\n            loss   = criterion(logits, batch[\"label\"])\n\n            v_loss += loss.item()\n            v_corr += (logits.argmax(1) == batch[\"label\"]).sum().item()\n            v_tot  += batch[\"label\"].size(0)\n\n    avg_val_loss = v_loss / len(val_loader)\n    val_acc      = v_corr / v_tot\n\n    val_losses.append(avg_val_loss)\n    val_accuracies.append(val_acc)\n    print(f\"Epoch {epoch} Val   Loss: {avg_val_loss:.4f} Acc: {val_acc:.2%}\")\n\n    # ‚Äî Save only the best model so far ‚Äî\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        best_path = os.path.join(checkpoint_dir, \"best_model_task1_rank.pt\")\n        torch.save(model.state_dict(), best_path)\n        print(f\"‚Üí New best model saved (epoch {epoch}, Acc: {val_acc:.2%})\")\n\nprint(\"Done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:24:41.245081Z","iopub.execute_input":"2025-05-10T18:24:41.245619Z","iopub.status.idle":"2025-05-10T20:24:40.807793Z","shell.execute_reply.started":"2025-05-10T18:24:41.245596Z","shell.execute_reply":"2025-05-10T20:24:40.806879Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 1:   0%|          | 0/428 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 1 Train Loss: 0.6267 Acc: 64.24%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val]   Epoch 1:   0%|          | 0/76 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Val   Loss: 0.6248 Acc: 65.01%\n‚Üí New best model saved (epoch 1, Acc: 65.01%)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 2:   0%|          | 0/428 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 2 Train Loss: 0.4174 Acc: 81.36%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val]   Epoch 2:   0%|          | 0/76 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 2 Val   Loss: 0.7584 Acc: 64.34%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 3:   0%|          | 0/428 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 3 Train Loss: 0.1819 Acc: 93.44%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val]   Epoch 3:   0%|          | 0/76 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 3 Val   Loss: 1.2748 Acc: 62.35%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 4:   0%|          | 0/428 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 4 Train Loss: 0.0798 Acc: 98.30%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val]   Epoch 4:   0%|          | 0/76 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 4 Val   Loss: 1.9691 Acc: 64.51%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 5:   0%|          | 0/428 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 5 Train Loss: 0.0476 Acc: 99.06%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val]   Epoch 5:   0%|          | 0/76 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 5 Val   Loss: 2.0675 Acc: 64.01%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 6:   0%|          | 0/428 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 6 Train Loss: 0.0434 Acc: 99.27%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val]   Epoch 6:   0%|          | 0/76 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 6 Val   Loss: 2.2489 Acc: 65.67%\n‚Üí New best model saved (epoch 6, Acc: 65.67%)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 7:   0%|          | 0/428 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 7 Train Loss: 0.0350 Acc: 99.62%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val]   Epoch 7:   0%|          | 0/76 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 7 Val   Loss: 2.1971 Acc: 65.01%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 8:   0%|          | 0/428 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 8 Train Loss: 0.0363 Acc: 99.44%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val]   Epoch 8:   0%|          | 0/76 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 8 Val   Loss: 2.0283 Acc: 63.85%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 9:   0%|          | 0/428 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 9 Train Loss: 0.0411 Acc: 99.21%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val]   Epoch 9:   0%|          | 0/76 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 9 Val   Loss: 2.7228 Acc: 62.69%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 10:   0%|          | 0/428 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 10 Train Loss: 0.0286 Acc: 99.59%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val]   Epoch 10:   0%|          | 0/76 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 10 Val   Loss: 2.4272 Acc: 64.68%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 11:   0%|          | 0/428 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 11 Train Loss: 0.0343 Acc: 99.27%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val]   Epoch 11:   0%|          | 0/76 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 11 Val   Loss: 2.9820 Acc: 62.85%\nDone!\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model = MemeClassifier(num_classes=2).to(device)\n# criterion = nn.CrossEntropyLoss()  # Standard loss function for classification\n# #optimizer = optim.AdamW(model.parameters(), lr=2e-5)  # Optimizer\n# optimizer = optim.AdamW([\n#     {\"params\": model.blip.parameters(), \"lr\": 5e-6},  # üîπ Lower LR for BLIP\n#     {\"params\": model.bert.parameters(), \"lr\": 5e-6},  # üîπ Lower LR for BERT\n#     {\"params\": model.fc.parameters(), \"lr\": 1e-4}     # üîπ Higher LR for classifier\n# ], weight_decay=0.01)\n\n\n\n\n\n######################################################################## every time ##############################################################","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:24:26.342833Z","iopub.status.idle":"2025-05-10T18:24:26.343082Z","shell.execute_reply.started":"2025-05-10T18:24:26.342969Z","shell.execute_reply":"2025-05-10T18:24:26.342979Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport pandas as pd\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom transformers import AutoTokenizer, AutoModel, BlipProcessor, BlipForConditionalGeneration\n\n# ‚îÄ‚îÄ‚îÄ Configuration ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nCHECKPOINT = \"/kaggle/input/checkpoint-task1-rank/best_model_task1_rank.pt\"\nCSV_PATH   = \"/kaggle/input/testing-clef-testcases/testing_data_1605.csv\"\nBATCH_SIZE = 8\nTEST_CASE  = \"EXIST2025\"  # if you need to change this label, set it here\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ‚îÄ‚îÄ‚îÄ Re-instantiate Models & Tokenizers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n# BLIP for caption generation + vision\nblip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nblip_model     = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n\n# BERT\nbert_model  = AutoModel.from_pretrained(\"bert-base-multilingual-cased\").to(device)\ntokenizer   = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n\n# ‚îÄ‚îÄ‚îÄ Dataset & Collate ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\ndef process_image(path):\n    img = Image.open(path).convert(\"RGB\")\n    tensor = blip_processor(images=img, return_tensors=\"pt\")[\"pixel_values\"]\n    return tensor.squeeze(0)\n\nclass TestDataset(Dataset):\n    def __init__(self, csv_path):\n        self.df = pd.read_csv(csv_path)\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, i):\n        row = self.df.iloc[i]\n        return {\n            \"id\":        str(row[\"id_EXIST\"]),\n            \"image\":     process_image(row[\"path_memes\"]),\n            \"raw_text\":  row[\"text\"]\n        }\n\ndef collate_fn(batch):\n    images    = torch.stack([x[\"image\"]    for x in batch], dim=0).to(device)\n    raw_texts = [             x[\"raw_text\"] for x in batch]\n    # only original text needs tokenization here\n    encoded   = tokenizer(raw_texts,\n                          padding=\"max_length\",\n                          truncation=True,\n                          max_length=96,\n                          return_tensors=\"pt\").to(device)\n    ids       = [x[\"id\"] for x in batch]\n    return images, encoded, raw_texts, ids\n\n# ‚îÄ‚îÄ‚îÄ Model Definition (same as during training) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nimport torch.nn as nn\n\nclass AttentionPooling(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        self.attn = nn.Linear(hidden_dim, 1)\n    def forward(self, x):\n        weights = torch.softmax(self.attn(x), dim=1)\n        return torch.sum(weights * x, dim=1)\n\nclass MemeClassifier(nn.Module):\n    def __init__(self, num_classes=2):\n        super().__init__()\n        self.blip = BlipForConditionalGeneration.from_pretrained(\n            \"Salesforce/blip-image-captioning-base\"\n        ).to(device)\n        self.bert = AutoModel.from_pretrained(\n            \"bert-base-multilingual-cased\"\n        ).to(device)\n        vh = self.blip.vision_model.config.hidden_size\n        th = self.bert.config.hidden_size\n        self.attn_pool = AttentionPooling(th)\n        self.classifier = nn.Sequential(\n            nn.Linear(vh + 2*th, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, images, text_inputs, raw_texts):\n        # vision features\n        vis      = self.blip.vision_model(pixel_values=images).last_hidden_state\n        img_feat = vis[:, 0, :]\n        # generate captions\n        self.blip.eval()\n        with torch.no_grad():\n            gen_ids = self.blip.generate(\n                pixel_values=images,\n                max_length=16, num_beams=1, early_stopping=True\n            )\n        self.blip.train()\n        caps = blip_processor.batch_decode(gen_ids, skip_special_tokens=True)\n        # encode captions\n        cap_inputs = tokenizer(caps, padding=True, truncation=True,\n                               max_length=96, return_tensors=\"pt\").to(device)\n        cap_hid    = self.bert(**cap_inputs).last_hidden_state\n        cap_feat   = self.attn_pool(cap_hid)\n        # encode original text\n        orig_hid  = self.bert(**text_inputs).last_hidden_state\n        orig_feat = self.attn_pool(orig_hid)\n        # concat + classify\n        combined  = torch.cat([img_feat, cap_feat, orig_feat], dim=1)\n        return self.classifier(combined)\n\n# ‚îÄ‚îÄ‚îÄ Load Checkpoint ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nmodel = MemeClassifier(num_classes=2).to(device)\nmodel.load_state_dict(torch.load(CHECKPOINT, map_location=device))\nmodel.eval()\n\n# ‚îÄ‚îÄ‚îÄ Inference ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\ntest_ds = TestDataset(CSV_PATH)\ntest_loader = DataLoader(test_ds,\n                         batch_size=BATCH_SIZE,\n                         shuffle=False,\n                         collate_fn=collate_fn)\n\nresults = []\nwith torch.no_grad():\n    for images, txt_inputs, raw_texts, ids in test_loader:\n        logits = model(images, txt_inputs, raw_texts)\n        preds  = logits.argmax(dim=1).cpu().tolist()\n        for uid, p in zip(ids, preds):\n            results.append({\n                \"test_case\": TEST_CASE,\n                \"id\":        uid,\n                \"value\":     \"YES\" if p==1 else \"NO\"\n            })\n\n# ‚îÄ‚îÄ‚îÄ Dump JSON ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nout_path = \"/kaggle/working/predictions.json\"\nwith open(out_path, \"w\") as f:\n    json.dump(results, f, indent=2)\n\nprint(f\"Wrote {len(results)} predictions to {out_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T18:24:33.576921Z","iopub.execute_input":"2025-05-16T18:24:33.577568Z","iopub.status.idle":"2025-05-16T18:27:49.783045Z","shell.execute_reply.started":"2025-05-16T18:24:33.577529Z","shell.execute_reply":"2025-05-16T18:27:49.782303Z"}},"outputs":[{"name":"stderr","text":"2025-05-16 18:24:48.058301: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747419888.253792      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747419888.316456      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c34fa43bb9a41999b58a24bf6f144c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"578641649e71491da54e2ec803527df1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0aa7e55ac9a47d8ba3eec326f29387b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1daedd0d3262414ea2da3fea415a7896"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bfa6eb9487a476893bc7fefecacf8ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"310087bb99a24a43a990226bd06d9d5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00c99178a23f4397860cf10505f2f413"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dcaad34fdc64bc99bb5be3060ec4c8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ad126ec930740dea1fa2b66cdb0d68f"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3089b31f304f49698f5d122ef9c54798"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d2234b559504ac8af3267e6f8d21e2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d677578ae6d049958cae7fe097b75795"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9a8f58bcbb1469687af8842f6d508a3"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_31/400722942.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(CHECKPOINT, map_location=device))\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Wrote 1053 predictions to /kaggle/working/predictions.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\n\n# 1) Load your original test file\norig_df = pd.read_csv(\"/kaggle/input/testing-clef-testcases/testing_data_1605.csv\")\n\n# 2) Load the JSON predictions\npreds = pd.read_json(\"/kaggle/input/task1-prediction/task1_rank_prediction.json\")\n\n# 3) Prepare preds: rename 'value'‚Üí'label', ensure ID types match\npreds = preds.rename(columns={\"value\": \"label\", \"id\": \"id_EXIST\"})\npreds[\"id_EXIST\"] = preds[\"id_EXIST\"].astype(orig_df[\"id_EXIST\"].dtype)\n\n# 4) Merge into the original DataFrame\nmerged = orig_df.merge(\n    preds[[\"id_EXIST\", \"label\"]],\n    on=\"id_EXIST\",\n    how=\"left\"\n)\n\n# 5) Save out\nout_path = \"testing_with_labels.csv\"\nmerged.to_csv(out_path, index=False)\nprint(f\"Wrote {len(merged)} rows with labels to {out_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:04:00.666166Z","iopub.execute_input":"2025-05-16T19:04:00.666535Z","iopub.status.idle":"2025-05-16T19:04:00.738909Z","shell.execute_reply.started":"2025-05-16T19:04:00.666511Z","shell.execute_reply":"2025-05-16T19:04:00.738320Z"}},"outputs":[{"name":"stdout","text":"Wrote 1053 rows with labels to testing_with_labels.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# import torch\n# import os\n\n# # Create checkpoint directory if it doesn't exist\n# checkpoint_dir = '/kaggle/working'\n# os.makedirs(checkpoint_dir, exist_ok=True)\n\n# num_epochs = 25\n# best_val_accuracy = 0  # Keep track of the best validation accuracy\n\n# # Lists to store training & validation accuracy/loss\n# train_losses, val_losses = [], []\n# train_accuracies, val_accuracies = [], []\n\n# for epoch in range(num_epochs):\n#     ###################################\n#     # -------- Training Phase --------#\n#     ###################################\n#     model.train()\n#     train_loss, correct_train, total_train = 0, 0, 0\n    \n#     for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n#         images, texts, labels = batch[\"image\"], batch[\"text\"], batch[\"label\"]\n\n#         # üîπ Ensure Correct Shape for BLIP\n#         if len(images.shape) == 3:\n#             images = images.unsqueeze(0)\n\n#         # üîπ Move Data to Device (CPU/GPU)\n#         images, labels = images.to(device), labels.to(device)\n#         texts = {key: val.to(device) for key, val in texts.items()}  # Move text tensors to device\n\n#         optimizer.zero_grad()\n#         outputs = model(images, texts)  # Forward Pass\n#         loss = criterion(outputs, labels)  # Compute Loss\n#         loss.backward()  # Backpropagation\n#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient Clipping\n#         optimizer.step()  # Update Weights\n        \n#         train_loss += loss.item()\n#         correct_train += (outputs.argmax(1) == labels).sum().item()\n#         total_train += labels.size(0)\n    \n#     train_accuracy = correct_train / total_train\n#     avg_train_loss = train_loss / len(train_loader)\n\n#     train_losses.append(avg_train_loss)\n#     train_accuracies.append(train_accuracy)\n    \n#     ######################################\n#     # -------- Validation Phase --------###\n#     #####################################\n#     model.eval()\n#     val_loss, correct_val, total_val = 0, 0, 0\n    \n#     with torch.no_grad():\n#         for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n#             images, texts, labels = batch[\"image\"], batch[\"text\"], batch[\"label\"]\n\n#             # üîπ Ensure Correct Shape for BLIP\n#             if len(images.shape) == 3:\n#                 images = images.unsqueeze(0)\n\n#             # üîπ Move Data to Device\n#             images, labels = images.to(device), labels.to(device)\n#             texts = {key: val.to(device) for key, val in texts.items()}  # Move text tensors to device\n\n#             outputs = model(images, texts)\n#             loss = criterion(outputs, labels)\n\n#             val_loss += loss.item()\n#             correct_val += (outputs.argmax(1) == labels).sum().item()\n#             total_val += labels.size(0)\n\n#     val_accuracy = correct_val / total_val\n#     avg_val_loss = val_loss / len(val_loader)\n\n#     val_losses.append(avg_val_loss)\n#     val_accuracies.append(val_accuracy)\n\n#     print(f\"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2%} | \"\n#           f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2%}\")\n\n#     # Save the model if it has the highest validation accuracy so far\n#     if val_accuracy > best_val_accuracy:\n#         best_val_accuracy = val_accuracy\n#         best_model_path = os.path.join(checkpoint_dir, f\"best_model_epoch_{epoch+1}.pt\")\n#         torch.save(model.state_dict(), best_model_path)\n#         print(f\"Saved best model with Val Acc: {val_accuracy:.2%} at epoch {epoch+1}\")\n\n# print(\"Training & Validation Complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:24:26.344023Z","iopub.status.idle":"2025-05-10T18:24:26.344362Z","shell.execute_reply.started":"2025-05-10T18:24:26.344173Z","shell.execute_reply":"2025-05-10T18:24:26.344191Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls /kaggle/working/checkpoints","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:24:26.345762Z","iopub.status.idle":"2025-05-10T18:24:26.345992Z","shell.execute_reply.started":"2025-05-10T18:24:26.345892Z","shell.execute_reply":"2025-05-10T18:24:26.345902Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import shutil\n# import zipfile\n\n# # Define paths\n# model_path = \"/kaggle/working/checkpoints/best_model_epoch_2.pt\"\n# zip_path = \"/kaggle/working/best_model.zip\"\n\n# # Zip the model file\n# with zipfile.ZipFile(zip_path, \"w\") as zipf:\n#     zipf.write(model_path, arcname=\"best_model_epoch_2.pt\")\n\n# # Move to working directory\n# shutil.move(zip_path, \"/kaggle/working/best_model.zip\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:24:26.347430Z","iopub.status.idle":"2025-05-10T18:24:26.347748Z","shell.execute_reply.started":"2025-05-10T18:24:26.347608Z","shell.execute_reply":"2025-05-10T18:24:26.347623Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## End to End Pipeline","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import gradio as gr\n# import torch\n# import torchvision.transforms as transforms\n# from PIL import Image\n# import pytesseract\n# from transformers import AutoTokenizer\n\n\n# checkpoint_path = '/kaggle/input/task1-checkpoint-blip-and-bert/best_model_epoch_2.pt'\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n# model.to(device)\n# model.eval()\n\n# # Load tokenizer for text preprocessing\n# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# # Define image transformations\n# transform = transforms.Compose([\n#     transforms.Resize((224, 224)),\n#     transforms.ToTensor(),\n# ])\n\n# def extract_text_from_image(image):\n#     \"\"\"Extract text from meme image using Tesseract OCR.\"\"\"\n#     text = pytesseract.image_to_string(image)\n#     return text.strip()\n\n# def classify_meme(image):\n#     \"\"\"Classify whether a meme is sexist or not.\"\"\"\n#     # Extract text from the image\n#     extracted_text = extract_text_from_image(image)\n    \n#     # Preprocess image\n#     image = transform(image).unsqueeze(0).to(device)\n    \n#     # Tokenize text\n#     encoded_text = tokenizer(extracted_text, return_tensors=\"pt\", padding=True, truncation=True)\n#     encoded_text = {key: val.to(device) for key, val in encoded_text.items()}\n    \n#     # Run inference\n#     with torch.no_grad():\n#         output = model(image, encoded_text)\n#         prediction = torch.argmax(output, dim=1).item()\n    \n#     # Map prediction to label\n#     label = \"Sexist Meme\" if prediction == 1 else \"Not Sexist Meme\"\n#     return label\n\n# # Gradio Interface\n# demo = gr.Interface(\n#     fn=classify_meme,\n#     inputs=gr.Image(type=\"pil\"),\n#     outputs=\"text\",\n#     title=\"Meme Classifier\",\n#     description=\"Upload a meme to classify whether it is sexist or not.\",\n# )\n\n# demo.launch(share=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:24:26.348941Z","iopub.status.idle":"2025-05-10T18:24:26.349261Z","shell.execute_reply.started":"2025-05-10T18:24:26.349103Z","shell.execute_reply":"2025-05-10T18:24:26.349117Z"}},"outputs":[],"execution_count":null}]}